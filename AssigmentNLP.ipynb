{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AssigmentNLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtDc_pbU1TCz",
        "outputId": "e5796e74-0d11-4f55-8966-5af73d9c3610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learned_sents = brown.sents(categories='learned')"
      ],
      "metadata": {
        "id": "1X3OUPCmRc_s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = open(\"data.txt\", \"w\")\n",
        "\n",
        "for sents in learned_sents:\n",
        "  text_file.write(\" \".join(sents))\n",
        "  text_file.write(\"\\n\")\n",
        "\n",
        "text_file.close()"
      ],
      "metadata": {
        "id": "AfRem9mECnPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trigram(corpus,str):\n",
        "  dict = {}\n",
        "  len_of_str = len(str)\n",
        "  for s in corpus:\n",
        "    if len(s) > 2:\n",
        "      for i in range(0,len(s)):\n",
        "        if i == len(s)-2:\n",
        "          break\n",
        "        if s[i].lower() == str[len_of_str-2].lower() and s[i+1].lower() == str[len_of_str-1].lower():\n",
        "          dict[s[i+2]] = dict.get(s[i+2], 0) + 1\n",
        "\n",
        "  list_of_words = sorted(dict.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)\n",
        "  \n",
        "  if len(list_of_words) > 4:\n",
        "    return list_of_words[0:5]\n",
        "  else:\n",
        "    return list_of_words\n",
        "\n"
      ],
      "metadata": {
        "id": "lWB0AuZBHpmJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  x = input('\\t')\n",
        "  x = \" \".join(x.split())\n",
        "\n",
        "  if(len(x) == 0):\n",
        "    break\n",
        "\n",
        "  list_of_seq = trigram(learned_sents, re.split(\"\\\\W+\", x))\n",
        " \n",
        "  for tuble in list_of_seq:\n",
        "    print(x, tuble[0])\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhLs2oy2QiK6",
        "outputId": "bbf9c146-78f5-4daf-cf2e-4a7a449b2130"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tthe operator\n",
            "the operator Af\n",
            "the operator is\n",
            "the operator induced\n",
            "the operator p\n",
            "the operator must\n",
            "\n",
            "\n",
            "\tis often\n",
            "is often stated\n",
            "is often called\n",
            "is often shortened\n",
            "is often encountered\n",
            "is often accomplished\n",
            "\n",
            "\n",
            "\t\n"
          ]
        }
      ]
    }
  ]
}